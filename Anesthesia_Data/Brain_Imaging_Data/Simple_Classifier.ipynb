{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e9f3ffa4-2d95-4158-8ac6-92b145e05e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.utils import shuffle\n",
    "import xgboost as xgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7a7382e9-201b-4832-b71b-17eba2db91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = [\"m292\", \"m294\"]\n",
    "label_list  = [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "57398fdc-4a59-4c3f-93b7-5ee5c5b43825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_and_concatenate_datasets(subjects, list_of_filenames, parent_directory):\n",
    "    \"\"\"\n",
    "    Load feature DataFrames for specified subjects.\n",
    "\n",
    "    Args:\n",
    "    - subjects (list): List of subject names.\n",
    "    - list_of_filenames (list): List of filenames to load for each subject.\n",
    "    - parent_directory (str): Parent directory where subject data is stored.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing subject feature DataFrames.\n",
    "    - list: List of all labels across subjects.\n",
    "    - pd.DataFrame: Concatenated feature DataFrame.\n",
    "    \"\"\"\n",
    "    subject_feature_dfs = {}\n",
    "    all_labels = []\n",
    "\n",
    "    for subject in subjects:\n",
    "        single_data_frames = []\n",
    "\n",
    "        for filename in list_of_filenames:\n",
    "            path = os.path.join(parent_directory, \"Features\", subject, filename)\n",
    "            if os.path.exists(path):\n",
    "                single_data_frames.append(pd.read_csv(path))\n",
    "\n",
    "        path = os.path.join(parent_directory, \"Features\", subject, \"Traditional_Features.csv\")\n",
    "        if os.path.exists(path):\n",
    "            single_data_frames.append(pd.read_csv(path))\n",
    "\n",
    "        combined_df = pd.concat(single_data_frames, axis=1)\n",
    "\n",
    "        # Drop unnamed columns\n",
    "        combined_df = combined_df.loc[:, ~combined_df.columns.str.contains('unnamed', case=False)]\n",
    "\n",
    "        # Remove _left columns\n",
    "        combined_df.drop(combined_df.filter(like='_left').columns, axis=1, inplace=True)\n",
    "\n",
    "        # Remove duplicates based on Label column\n",
    "        combined_df.drop_duplicates(subset='Label', inplace=True)\n",
    "\n",
    "        # Add Subject column\n",
    "        combined_df['Subject'] = subjects.index(subject)\n",
    "\n",
    "        subject_feature_dfs[subject] = combined_df\n",
    "\n",
    "        all_labels.extend(combined_df['Label'])\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    feature_df = pd.concat(subject_feature_dfs.values(), ignore_index=True)\n",
    "\n",
    "    # Drop Label column\n",
    "    feature_df.drop(columns=[\"Label\"], inplace=True)\n",
    "\n",
    "    return subject_feature_dfs, all_labels, feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8f2bb167-abf6-416c-acf2-32dcb619dc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_filenames = [\"Topological_Summary_Statistics.csv\", \"Signature_Statistics.csv\", \"Advanced_Features.csv\"]\n",
    "\n",
    "subject_feature_dfs, all_labels, feature_df = import_and_concatenate_datasets(subject_list, list_of_filenames, parent_directory=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ef8fb-0483-47c0-a098-4765de47b4f5",
   "metadata": {},
   "source": [
    "# Save Concatenated Features for Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "caa3972e-ba2f-4b3c-ad3a-181bb16b747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe for data exploration\n",
    "feature_df.to_csv(\"Features/All_Features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f333182-30ea-498e-9fe7-030de10a57e8",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e0b70ae8-9494-4114-9f9c-cd786af04d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indices_all_subjects(subject_list):\n",
    "    \"\"\"\n",
    "    Load train, validation, and test set indices for all subjects.\n",
    "    \n",
    "    Args:\n",
    "    - subject_list (list): List of subject names.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing train indices for all subjects.\n",
    "    - dict: Dictionary containing validation indices for all subjects.\n",
    "    - dict: Dictionary containing test indices for all subjects.\n",
    "    \"\"\"\n",
    "    train_indices = {}\n",
    "    validation_indices = {}\n",
    "    test_indices_dict_all_subjects = {}\n",
    "\n",
    "    for subject in subject_list:\n",
    "        # Train indices\n",
    "        train_indices[subject] = np.load(\"../Time_Series/Train_Test_Splitting/\"+str(subject)+\"/Train_Indices_All_Labels_All_Folds.npy\", allow_pickle=True).item()\n",
    "\n",
    "        # Validation indices\n",
    "        validation_indices[subject] = np.load(\"../Time_Series/Train_Test_Splitting/\"+str(subject)+\"/Validation_Indices_All_Labels_All_Folds.npy\", allow_pickle=True).item()\n",
    "\n",
    "        # Final Test set indices\n",
    "        test_indices_dict_all_subjects[subject] = np.load(\"../Time_Series/Train_Test_Splitting/\"+str(subject)+\"/Final_Test_Set_Indices_All_Labels.npy\", allow_pickle=True).item()\n",
    "\n",
    "    return train_indices, validation_indices, test_indices_dict_all_subjects\n",
    "\n",
    "train_indices, validation_indices, test_indices = load_indices_all_subjects(subject_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f93452bb-c1f3-44ab-a6d4-100d8faa182d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index([14, 22, 70, 56, 37, 39, 17, 32, 67, 51, 33, 15, 45, 59, 60, 57, 69, 16,\\n       40, 66,  6, 48,  0, 47, 46, 29, 26, 49, 13, 38, 72, 71, 36, 50, 20, 10,\\n       18, 42, 35,  1, 11,  4,  9, 19, 62, 54, 43],\\n      dtype='int64')] are in the [index]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 77\u001b[0m\n\u001b[1;32m     71\u001b[0m             features_dfs_all_folds[fold_key]\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m features_dfs_all_folds, labels_all_folds\n\u001b[0;32m---> 77\u001b[0m train_features_dfs_all_folds, train_labels_all_folds \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_dataframe_with_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m validation_features_dfs_all_folds, validation_labels_all_folds \u001b[38;5;241m=\u001b[39m filter_dataframe_with_indices(feature_df, all_labels, validation_indices)\n",
      "Cell \u001b[0;32mIn[125], line 56\u001b[0m, in \u001b[0;36mfilter_dataframe_with_indices\u001b[0;34m(feature_df, all_labels, indices_dict_all_subjects)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, fold_key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indices_dict_all_subjects[subject][label_key]\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m     53\u001b[0m     set_indices_in_filtered_df \u001b[38;5;241m=\u001b[39m indices_dict_all_subjects[subject][label_key][fold_key]\n\u001b[0;32m---> 56\u001b[0m     feature_df_with_set_indices \u001b[38;5;241m=\u001b[39m \u001b[43mfiltered_label_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mset_indices_in_filtered_df\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     58\u001b[0m     labels_with_set_indices[fold_key]\u001b[38;5;241m.\u001b[39mextend([label]\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(feature_df_with_set_indices\u001b[38;5;241m.\u001b[39mindex))\n\u001b[1;32m     61\u001b[0m     features_dfs_for_subject[fold_key] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([features_dfs_for_subject[fold_key], feature_df_with_set_indices], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/pandas/core/indexing.py:1153\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1150\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1152\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/pandas/core/indexing.py:1382\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1380\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_iterable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/pandas/core/indexing.py:1322\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1324\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1325\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/pandas/core/indexing.py:1520\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1517\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1518\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1520\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/pandas/core/indexes/base.py:6175\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6174\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([14, 22, 70, 56, 37, 39, 17, 32, 67, 51, 33, 15, 45, 59, 60, 57, 69, 16,\\n       40, 66,  6, 48,  0, 47, 46, 29, 26, 49, 13, 38, 72, 71, 36, 50, 20, 10,\\n       18, 42, 35,  1, 11,  4,  9, 19, 62, 54, 43],\\n      dtype='int64')] are in the [index]\""
     ]
    }
   ],
   "source": [
    "def filter_dataframe_with_indices(feature_df, all_labels, indices_dict_all_subjects):\n",
    "    \"\"\"\n",
    "    indices_dict_all_subjects: Dictionary. Structure {subject: {label: {fold: indices list}}}.\n",
    "    The indices list is a list of indices within each \"label\" dataframe (for each subject). If there are\n",
    "    71 segments of each label (for a subject), then the indices list would be a subset of range(0, 71).\n",
    "    \"\"\"\n",
    "\n",
    "    features_dfs_all_folds = {}\n",
    "    labels_all_folds = {}\n",
    "\n",
    "    # Initialize dictionarys with folds as keys and the train/validation sets/ their labels as values\n",
    "    subject = list(indices_dict_all_subjects.keys())[0]\n",
    "    for fold, fold_key in enumerate(indices_dict_all_subjects[subject][list(indices_dict_all_subjects[subject].keys())[0]]):\n",
    "        features_dfs_all_folds[fold_key] = pd.DataFrame()\n",
    "        labels_all_folds[fold_key] = []\n",
    "\n",
    "    \n",
    "    for subject_idx, subject in enumerate(indices_dict_all_subjects.keys()):\n",
    "\n",
    "        # dictionarys with folds as labels and the train/validation sets/ their labels as values \n",
    "        labels_with_set_indices = {}\n",
    "        features_dfs_for_subject = {}\n",
    "\n",
    "        filtered_subject_df = feature_df[feature_df[\"Subject\"] == subject_idx]\n",
    "\n",
    "        indices_filtered_subject_df = filtered_subject_df.index # Save indices for labels\n",
    "        labels_for_subject = [all_labels[idx] for idx in indices_filtered_subject_df]\n",
    "        \n",
    "        filtered_subject_df = filtered_subject_df.reset_index()\n",
    "\n",
    "\n",
    "        for fold, fold_key in enumerate(indices_dict_all_subjects[subject][list(indices_dict_all_subjects[subject].keys())[0]]):\n",
    "            \n",
    "            features_dfs_for_subject[fold_key] = pd.DataFrame()\n",
    "            labels_with_set_indices[fold_key] = []\n",
    "\n",
    "        \n",
    "\n",
    "        for label, label_key in enumerate(train_indices[subject].keys()):\n",
    "\n",
    "            indices_of_label_within_subject_dataframe = [index for index, value in enumerate(labels_for_subject) if value == label]\n",
    "\n",
    "\n",
    "            filtered_label_df = filtered_subject_df.loc[indices_of_label_within_subject_dataframe]\n",
    "\n",
    "            #filtered_label_df.drop(columns = [\"level_0\"], inplace = True)\n",
    "\n",
    "            filtered_label_df = filtered_label_df.reset_index()\n",
    "\n",
    "\n",
    "            for fold, fold_key in enumerate(indices_dict_all_subjects[subject][label_key].keys()):\n",
    "\n",
    "                set_indices_in_filtered_df = indices_dict_all_subjects[subject][label_key][fold_key]\n",
    "\n",
    "\n",
    "                feature_df_with_set_indices = filtered_label_df.loc[set_indices_in_filtered_df]\n",
    "\n",
    "                labels_with_set_indices[fold_key].extend([label]*len(feature_df_with_set_indices.index))\n",
    "                \n",
    "        \n",
    "                features_dfs_for_subject[fold_key] = pd.concat([features_dfs_for_subject[fold_key], feature_df_with_set_indices], ignore_index=True)\n",
    "\n",
    "        \n",
    "        for fold, fold_key in enumerate(indices_dict_all_subjects[subject][list(indices_dict_all_subjects[subject].keys())[0]]):\n",
    "            features_dfs_all_folds[fold_key] =  pd.concat([features_dfs_for_subject[fold_key], features_dfs_for_subject[fold_key]], ignore_index=True)\n",
    "\n",
    "            labels_all_folds[fold_key].extend(labels_with_set_indices[fold_key])\n",
    "            \n",
    "            # Postprocessing\n",
    "            features_dfs_all_folds[fold_key].drop(columns = [\"level_0\"], inplace = True)\n",
    "            features_dfs_all_folds[fold_key].drop(columns = [\"index\"], inplace = True)\n",
    "\n",
    "\n",
    "    \n",
    "    return features_dfs_all_folds, labels_all_folds\n",
    "\n",
    "train_features_dfs_all_folds, train_labels_all_folds = filter_dataframe_with_indices(feature_df, all_labels, train_indices)\n",
    "validation_features_dfs_all_folds, validation_labels_all_folds = filter_dataframe_with_indices(feature_df, all_labels, validation_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "162ffa1c-43a9-45d1-943c-6402a541407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_fold_dicts(train_features_dfs_all_folds, train_labels_all_folds, validation_features_dfs_all_folds, validation_labels_all_folds, n_folds = 5):\n",
    "    \"\"\"\n",
    "    Initialize dictionaries with folds as keys and assign features and labels accordingly.\n",
    "    \n",
    "    Args:\n",
    "    - train_features_dfs_all_folds (dict): Dictionary containing training features for all folds.\n",
    "    - train_labels_all_folds (dict): Dictionary containing training labels for all folds.\n",
    "    - validation_features_dfs_all_folds (dict): Dictionary containing validation features for all folds.\n",
    "    - validation_labels_all_folds (dict): Dictionary containing validation labels for all folds.\n",
    "    - n_folds (int): Number of folds.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Dictionary containing training features for each fold.\n",
    "    - dict: Dictionary containing training labels for each fold.\n",
    "    - dict: Dictionary containing validation features for each fold.\n",
    "    - dict: Dictionary containing validation labels for each fold.\n",
    "    \"\"\"\n",
    "    X_train = {}\n",
    "    y_train = {}\n",
    "    X_test = {}\n",
    "    y_test = {}\n",
    "\n",
    "    for fold in range(n_folds):\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Shuffle indices\n",
    "        indices_train = np.random.permutation(len(train_features_dfs_all_folds[\"Fold_\" + str(fold)]))\n",
    "        indices_test = np.random.permutation(len(validation_features_dfs_all_folds[\"Fold_\" + str(fold)]))\n",
    "\n",
    "        # Shuffle rows of X_train[fold] and y_train[fold]\n",
    "        X_train_fold = train_features_dfs_all_folds[\"Fold_\" + str(fold)].iloc[indices_train]\n",
    "        y_train_fold = [train_labels_all_folds[\"Fold_\" + str(fold)][index] for index in indices_train]\n",
    "\n",
    "        # Shuffle rows of X_test[fold] and y_test[fold]\n",
    "        X_test_fold = validation_features_dfs_all_folds[\"Fold_\" + str(fold)].iloc[indices_test]\n",
    "        y_test_fold = [validation_labels_all_folds[\"Fold_\" + str(fold)][index] for index in indices_test]\n",
    "\n",
    "        X_train[fold] = X_train_fold\n",
    "        y_train[fold] = y_train_fold\n",
    "        X_test[fold] = X_test_fold\n",
    "        y_test[fold] = y_test_fold\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = initialize_fold_dicts(train_features_dfs_all_folds, train_labels_all_folds, validation_features_dfs_all_folds, validation_labels_all_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bcf0c-eff9-4be3-9112-87d2eae8a676",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "51a5995c-42e5-41c8-ad7a-a787e5e4029b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 0 : 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 1 : 0.7833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 2 : 0.7833333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 3 : 0.85\n",
      "Accuracy for fold 4 : 0.8909090909090909\n",
      "Mean Accuracy: 0.8215151515151515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "def train_rf_cross_validation(X_train, y_train, X_test, y_test, n_estimators=500, random_state=5):\n",
    "    \"\"\"\n",
    "    Train RandomForestClassifier using cross-validation and calculate mean accuracy.\n",
    "    \n",
    "    Args:\n",
    "    - X_train (dict): Dictionary containing training features for each fold.\n",
    "    - y_train (dict): Dictionary containing training labels for each fold.\n",
    "    - X_test (dict): Dictionary containing validation features for each fold.\n",
    "    - y_test (dict): Dictionary containing validation labels for each fold.\n",
    "    - n_estimators (int): Number of trees in the forest (default=900).\n",
    "    - random_state (int): Random seed (default=5).\n",
    "    \n",
    "    Returns:\n",
    "    - float: Mean accuracy across all folds.\n",
    "    \"\"\"\n",
    "    rf = RandomForestClassifier(random_state=random_state, n_estimators=n_estimators)\n",
    "    all_accuracies = []\n",
    "\n",
    "    for fold in range(len(X_train)):\n",
    "        rf.fit(X_train[fold], y_train[fold])\n",
    "        y_pred = rf.predict(X_test[fold])\n",
    "        accuracy = accuracy_score(y_pred, y_test[fold])\n",
    "        all_accuracies.append(accuracy)\n",
    "        print(\"Accuracy for fold\", fold, \":\", accuracy)\n",
    "\n",
    "    mean_accuracy = np.mean(all_accuracies)\n",
    "    print(\"Mean Accuracy:\", mean_accuracy)\n",
    "    pass\n",
    "\n",
    "train_rf_cross_validation(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf608d24-82fe-4a5a-a629-635bb7fb4036",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d606a402-8292-4525-8cca-f02726105433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 0 : 0.85\n",
      "Accuracy for fold 1 : 0.8\n",
      "Accuracy for fold 2 : 0.8333333333333334\n",
      "Accuracy for fold 3 : 0.8\n",
      "Accuracy for fold 4 : 0.8363636363636363\n",
      "Mean Accuracy: 0.8239393939393939\n"
     ]
    }
   ],
   "source": [
    "def train_xgb_cross_validation(X_train, y_train, X_test, y_test, seed=41):\n",
    "    \"\"\"\n",
    "    Train XGBoost Classifier using cross-validation and calculate mean accuracy.\n",
    "    \n",
    "    Args:\n",
    "    - X_train (dict): Dictionary containing training features for each fold.\n",
    "    - y_train (dict): Dictionary containing training labels for each fold.\n",
    "    - X_test (dict): Dictionary containing validation features for each fold.\n",
    "    - y_test (dict): Dictionary containing validation labels for each fold.\n",
    "    - seed (int): Random seed (default=41).\n",
    "    \n",
    "    Returns:\n",
    "    - float: Mean accuracy across all folds.\n",
    "    \"\"\"\n",
    "    model = xgb.XGBClassifier(seed=seed)\n",
    "    all_accuracies = []\n",
    "\n",
    "    for fold in range(len(X_train)):\n",
    "        # Remove duplicate columns\n",
    "        X_train[fold] = X_train[fold].loc[:, ~X_train[fold].columns.duplicated()]\n",
    "\n",
    "        model.fit(X_train[fold], y_train[fold])\n",
    "\n",
    "        X_test[fold] = X_test[fold].loc[:, ~X_test[fold].columns.duplicated()]\n",
    "\n",
    "        y_pred = model.predict(X_test[fold])\n",
    "        accuracy = accuracy_score(y_pred, y_test[fold])\n",
    "        all_accuracies.append(accuracy)\n",
    "        print(\"Accuracy for fold\", fold, \":\", accuracy)\n",
    "\n",
    "    mean_accuracy = np.mean(all_accuracies)\n",
    "    print(\"Mean Accuracy:\", mean_accuracy)\n",
    "    return mean_accuracy\n",
    "\n",
    "mean_accuracy_xgb = train_xgb_cross_validation(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04a3d5-a0e1-488b-becd-d6832adeb6f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
