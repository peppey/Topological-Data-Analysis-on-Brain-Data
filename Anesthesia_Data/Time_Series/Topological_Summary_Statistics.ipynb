{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a81b9c9-a2e9-46c2-8ae8-4a5f02dd0571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyedflib\n",
    "import statistics\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from gtda.time_series import SingleTakensEmbedding\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.diagrams import PersistenceEntropy, Amplitude, NumberOfPoints, ComplexPolynomial, PersistenceLandscape, HeatKernel, Silhouette, BettiCurve, PairwiseDistance, ForgetDimension\n",
    "from gtda.plotting import plot_point_cloud, plot_heatmap, plot_diagram\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from gtda.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "139f5d61-3e73-444d-8998-87fcecff5eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose if you want to look at EEG or EMG data\n",
    "\n",
    "data_type = \"EEG\" # Does not have an effect yet, will be added later when processing anesthesia data\n",
    "#data_type = \"EMG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "27021192-1231-4b27-8f52-a8cfa0f9ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose individuum\n",
    "\n",
    "subject = \"m292\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fd393cad-0556-4dd4-9dda-fbd1afb8bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [0, 1, 2, 3, 4]\n",
    "\n",
    "n_folds = 5 # This should be the same as in the file which creates PDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0c3d1854-aa34-491c-a97b-7cda1c72952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load persistence diagrams\n",
    "\n",
    "test_persistence_diagrams = {} # dictionary with labels as keys, persistence diagrams of the respective classes as values\n",
    "persistence_diagrams_for_cross_validation = {} # dictionary with labels as keys, persistence diagrams of the respective classes as values\n",
    "train_indices_dict_for_folds = {}\n",
    "validation_indices_dict_for_folds = {}\n",
    "\n",
    "\n",
    "for label in label_list:\n",
    "    # Initialize dictionary with folds as keys\n",
    "    train_indices_dict_for_folds[label] = {}\n",
    "    validation_indices_dict_for_folds[label] = {}\n",
    "\n",
    "\n",
    "for label in label_list:\n",
    "\n",
    "    #### Final test set ####\n",
    "    test_persistence_diagrams[label]  = np.load('Embeddings_and_Persistence_Diagrams/'+str(subject)+'/Test/'+str(data_type)+'/PD_Label'+str(label)+'.npy', \\\n",
    "    allow_pickle=True).item() # .item() to convert the dtype to dict again\n",
    "\n",
    "\n",
    "    #### Data for cross validation ####\n",
    "\n",
    "\n",
    "    # Import all train persistence diagrams #\n",
    "    persistence_diagrams_for_cross_validation[label] = np.load('Embeddings_and_Persistence_Diagrams/'+str(subject)+'/Train/'+str(data_type)+'/PD_Label_'+str(label)+'.npy', \\\n",
    "                allow_pickle=True).item() # .item() to convert the dtype to dict again\n",
    "\n",
    "    \n",
    "    # Import train and validation indices for all folds for each label\n",
    "    for fold in range(n_folds):\n",
    "        # Train Set\n",
    "        train_indices_dict_for_folds[label][fold] = np.load('Embeddings_and_Persistence_Diagrams/'+str(subject)+'/Train/'+str(data_type)+'/Fold_'+str(fold)+'/Train/Indices_Label_'+str(label)+'.npy', \\\n",
    "                allow_pickle=True)\n",
    "        \n",
    "        # Validation Set\n",
    "        validation_indices_dict_for_folds[label][fold] = np.load('Embeddings_and_Persistence_Diagrams/'+str(subject)+'/Train/'+str(data_type)+'/Fold_'+str(fold)+'/Validation/Indices_Label_'+str(label)+'.npy', \\\n",
    "                allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd32ce7-5e67-46a6-afb6-edc7a7d4190a",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bb3cc473-e759-4b8b-aba7-d70ccb57f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary_statistics(persistence_diagrams):\n",
    "    \"\"\"\n",
    "    Compute summary statistics of list of persistence diagrams\n",
    "\n",
    "    Parameters:\n",
    "    - persistence_diagrams (list): persistence diagrams\n",
    "\n",
    "    Returns:\n",
    "    Tuple of four lists:\n",
    "    - Persistence Entropy\n",
    "    - Persistence\n",
    "    - Betti Numbers\n",
    "    - Complex Polynomials\n",
    "    \"\"\"\n",
    "    \n",
    "    PE = PersistenceEntropy()\n",
    "    AM = Amplitude()\n",
    "    NP = NumberOfPoints()\n",
    "    CP = ComplexPolynomial(n_coefficients=1)\n",
    "\n",
    "    persistence_entropies = []\n",
    "    amplitudes = []\n",
    "    nos_points = []\n",
    "    complex_polynomials = []\n",
    "\n",
    "    for diagram in persistence_diagrams:\n",
    "        persistence_entropies.append(PE.fit_transform([diagram]))\n",
    "        amplitudes.append(AM.fit_transform([diagram]))\n",
    "        nos_points.append(NP.fit_transform([diagram]))\n",
    "        #complex_polynomials.append(CP.fit_transform([diagram]))\n",
    "\n",
    "    return persistence_entropies, amplitudes, nos_points, #complex_polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "c8b0ab85-43ec-4357-9d46-6d4e12f063f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dict for final test set with labels as key\n",
    "features_for_test_set = {}\n",
    "\n",
    "# Initialize dicts for the different folds set with labels as key\n",
    "features_for_cross_validation = {}\n",
    "\n",
    "for label in label_list:\n",
    "\n",
    "    # Final test set features\n",
    "    features_for_test_set[label] = compute_summary_statistics(test_persistence_diagrams[label][0])\n",
    "\n",
    "    # Initialize a dict for each label for the different folds\n",
    "    features_for_cross_validation[label] = {}\n",
    "\n",
    "    for PD_idx in persistence_diagrams_for_cross_validation[label][0].keys():\n",
    "        features_for_cross_validation[label][PD_idx] = \\\n",
    "            compute_summary_statistics(persistence_diagrams_for_cross_validation[label][0][PD_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7288defc-2cb8-4621-abd4-1e0a4a873de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_largest_persistence(persistence_diagrams):\n",
    "    \"\"\"\n",
    "    Computes persistence of the most prominent points of each dimension in each diagram\n",
    "\n",
    "    Parameters:\n",
    "    - persistence_diagrams (list): persistence diagrams\n",
    "\n",
    "    Returns:\n",
    "    List of 3 lists:\n",
    "    - List of the largest persistences of homology dimension 0\n",
    "    - List of the largest persistences of homology dimension 1\n",
    "    - List of the largest persistences of homology dimension 2 \n",
    "    \"\"\"\n",
    "\n",
    "    largest_persistences = [] # will contain 3 lists for the 3 homology dimensions\n",
    "    for homology_dimension in [0, 1, 2]:\n",
    "        largest_persistences_of_hom_dim = []\n",
    "        for diagram in persistence_diagrams:\n",
    "            # only look at holes of our homology dimension\n",
    "            condition = diagram[:, 2] == homology_dimension\n",
    "            filtered_diagram = diagram[condition]\n",
    "\n",
    "            if len(filtered_diagram) > 0:\n",
    "                differences = filtered_diagram[:, 1] - filtered_diagram[:, 0]\n",
    "                largest_persistences_of_hom_dim.append(np.max(differences))\n",
    "\n",
    "        largest_persistences.append(largest_persistences_of_hom_dim)\n",
    "\n",
    "    return largest_persistences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f7d02c88-07b8-4559-9bc9-f7506b6abd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dict for final test set with labels as key\n",
    "largest_persistences_for_test_set = {}\n",
    "\n",
    "# Initialize dicts for the different folds set with labels as key\n",
    "largest_persistences_for_cross_validation = {}\n",
    "\n",
    "for label in label_list:\n",
    "    \n",
    "    # Final test set features\n",
    "    largest_persistences_for_test_set[label] = compute_largest_persistence(test_persistence_diagrams[label][0])\n",
    "    \n",
    "    # Initialize a dict for each label for the different fold\n",
    "    largest_persistences_for_cross_validation[label] = {}\n",
    "\n",
    "    for PD_idx in persistence_diagrams_for_cross_validation[label][0].keys():\n",
    "        largest_persistences_for_cross_validation[label][PD_idx] = \\\n",
    "            compute_largest_persistence(persistence_diagrams_for_cross_validation[label][0][PD_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "620522f3-cf84-4e9e-abfb-f676b06ce6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features(features, train_indices_dict, test_indices_dict, label):\n",
    "    \n",
    "    train_features = {}\n",
    "    test_features = {}\n",
    "\n",
    "    \n",
    "    for fold_idx in range(len(train_indices_dict[0])):\n",
    "        \n",
    "        # Initalize feature dicts for each fold\n",
    "        train_features[fold_idx]  = [] # train set\n",
    "        test_features[fold_idx]  = [] # test set\n",
    "\n",
    "    # Loop through folds (only one fold in case of final test)\n",
    "    for fold_idx in range(len(train_indices_dict[label])):\n",
    "        # Loop through the first train data segments with our label\n",
    "        \n",
    "        for PD_idx in train_indices_dict[label][fold_idx]: \n",
    "            train_features[fold_idx].append(features[label][PD_idx][0])\n",
    "\n",
    "        for PD_idx in test_indices_dict[label][fold_idx]:\n",
    "            test_features[fold_idx].append(features[label][PD_idx][0])\n",
    "\n",
    "\n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "21bbb8a3-ed27-44aa-b6a2-2cd8b243fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split features into train and validation set for different folds\n",
    "\n",
    "train_features_for_cross_validation = {}\n",
    "validation_features_for_cross_validation = {}\n",
    "\n",
    "for label in label_list:\n",
    "    train_features_for_cross_validation[label], validation_features_for_cross_validation[label] = \\\n",
    "    split_features(features_for_cross_validation, train_indices_dict_for_folds, validation_indices_dict_for_folds, label = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "38a1876c-ab87-4cfc-92d7-88d13c6c9b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split largest persistence feature into train and validation set for different folds\n",
    "\n",
    "train_largest_persistences_for_cross_validation = {}\n",
    "validation_largest_persistences_for_cross_validation = {}\n",
    "\n",
    "for label in label_list:\n",
    "    train_largest_persistences_for_cross_validation[label], validation_largest_persistences_for_cross_validation[label] = \\\n",
    "    split_features(largest_persistences_for_cross_validation, train_indices_dict_for_folds, validation_indices_dict_for_folds, label = label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c925f5-d589-47b5-a701-287ecb8c3c05",
   "metadata": {},
   "source": [
    "# Concatenate Features to one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2faa621b-b1b3-4428-87e6-10f15d0364ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_column_in_matrix(matrix, i):\n",
    "    return [row[0][i] for row in matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "54d4bd2a-5a8c-4a90-ba3b-459bb13209d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_df(subject, data_type, persistence_entropies, amplitudes, nos_points, persistences, label, train):\n",
    "    \"\"\"\n",
    "    Create DataFrame for each label from features\n",
    "\n",
    "    Parameters:\n",
    "    - persistence_entropies (list): persistence entropies\n",
    "    - amplitudes (list): amplitudes\n",
    "    - nos_points (list): number of points\n",
    "    - label (int): Label for which we want to create a dataframe. 1, 3, 5 or 7.\n",
    "\n",
    "    Returns:\n",
    "    - Feature DataFrame (DataFrame)\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_df = pd.DataFrame()\n",
    "\n",
    "    # All 3 columns (corresponding to hole dimensions)\n",
    "    feature_df[str(data_type)+\"_Persistence Entropy_Dim_0\"] = choose_column_in_matrix(list(persistence_entropies), 0)\n",
    "    feature_df[str(data_type)+\"_Persistence Entropy_Dim_1\"] = choose_column_in_matrix(list(persistence_entropies), 1)\n",
    "    feature_df[str(data_type)+\"_Persistence Entropy_Dim_2\"] = choose_column_in_matrix(list(persistence_entropies), 2)\n",
    "\n",
    "    # All 3 columns (corresponding to hole dimensions)\n",
    "    feature_df[str(data_type)+\"_Amplitude_Dim_0\"] = choose_column_in_matrix(list(amplitudes), 0)\n",
    "    feature_df[str(data_type)+\"_Amplitude_Dim_1\"] = choose_column_in_matrix(list(amplitudes), 1)\n",
    "    feature_df[str(data_type)+\"_Amplitude_Dim_2\"] = choose_column_in_matrix(list(amplitudes), 2)\n",
    "\n",
    "    # All 3 columns (corresponding to hole dimensions)\n",
    "    feature_df[str(data_type)+\"_No_Points_Dim_0\"] = choose_column_in_matrix(list(nos_points), 0)\n",
    "    feature_df[str(data_type)+\"_No_Points_Dim_1\"] = choose_column_in_matrix(list(nos_points), 1)\n",
    "    feature_df[str(data_type)+\"_No_Points_Dim_2\"] = choose_column_in_matrix(list(nos_points), 2)\n",
    "\n",
    "\n",
    "    feature_df[str(data_type)+\"_Largest_Persistence_Dim_0\"] = persistences[0]\n",
    "    feature_df[str(data_type)+\"_Largest_Persistence_Dim_1\"] = persistences[1]\n",
    "    feature_df[str(data_type)+\"_Largest_Persistence_Dim_2\"] = persistences[2]\n",
    "\n",
    "    # Label\n",
    "    feature_df[\"Label\"] = label\n",
    "\n",
    "    # Subject\n",
    "    feature_df[\"Subject\"] = subject\n",
    "\n",
    "    feature_df[\"Train\"] = train\n",
    "\n",
    "\n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "659df5f4-0f62-4404-889f-3d29587e15af",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframes= {}\n",
    "\n",
    "train_dataframes = {}\n",
    "validation_dataframes = {}\n",
    "\n",
    "\n",
    "\n",
    "for label in label_list:\n",
    "    # Final test set\n",
    "    persistence_entropies = features_for_test_set[label][0]\n",
    "    amplitudes = features_for_test_set[label][1]\n",
    "    nos_points = features_for_test_set[label][2]\n",
    "\n",
    "    test_dataframes[label] = create_feature_df(subject, data_type, persistence_entropies, amplitudes, nos_points, \\\n",
    "                                                largest_persistences_for_test_set[label], label, False)\n",
    "\n",
    "    # Initialize dict with folds as key for each label\n",
    "    train_dataframes[label] = {}\n",
    "    validation_dataframes[label] = {}\n",
    "\n",
    "    # Folds\n",
    "    for fold in range(n_folds):\n",
    "        # Train sets\n",
    "        persistence_entropies = train_features_for_cross_validation[label][fold][0]\n",
    "        amplitudes = train_features_for_cross_validation[label][fold][1]\n",
    "        nos_points = train_features_for_cross_validation[label][fold][2]\n",
    "        \n",
    "        train_dataframes[label][fold] = create_feature_df(subject, data_type, persistence_entropies, amplitudes, nos_points, \\\n",
    "                                                train_largest_persistences_for_cross_validation[label][fold], label, False)\n",
    "\n",
    "\n",
    "        # Validation sets\n",
    "        persistence_entropies = validation_features_for_cross_validation[label][fold][0]\n",
    "        amplitudes = validation_features_for_cross_validation[label][fold][1]\n",
    "        nos_points = validation_features_for_cross_validation[label][fold][2]\n",
    "        \n",
    "        validation_dataframes[label][fold] = create_feature_df(subject, data_type, persistence_entropies, amplitudes, nos_points, \\\n",
    "                                                validation_largest_persistences_for_cross_validation[label][fold], label, False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "273e5219-0ca4-4fde-8ef2-bded94a33143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and save features of final test set\n",
    "test_feature_df = pd.concat([test_dataframes[0], test_dataframes[1], test_dataframes[2], test_dataframes[3], test_dataframes[4]], ignore_index=True)\n",
    "test_feature_df.to_csv(\"Features/\"+str(subject)+\"/Test/\"+str(data_type)+\"/Topological_Summary_Statistics.csv\")\n",
    "\n",
    "\n",
    "# Concatenate and save features of for single folds\n",
    "for fold in range(n_folds):\n",
    "    # Train\n",
    "    train_feature_df = pd.concat([test_dataframes[0], test_dataframes[1], test_dataframes[2], test_dataframes[3], test_dataframes[4]], ignore_index=True)\n",
    "    train_feature_df.to_csv(\"Features/\"+str(subject)+\"/Train/\"+str(data_type)+\"/Fold_\"+str(fold)+\"/Train/Topological_Summary_Statistics.csv\")\n",
    "\n",
    "    # Validation\n",
    "    validation_feature_df = pd.concat([test_dataframes[0], test_dataframes[1], test_dataframes[2], test_dataframes[3], test_dataframes[4]], ignore_index=True)\n",
    "    validation_feature_df.to_csv(\"Features/\"+str(subject)+\"/Train/\"+str(data_type)+\"/Fold_\"+str(fold)+\"/Validation/Topological_Summary_Statistics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fade42cd-9306-4dfb-a31c-a4d9c9933235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
