{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f3ffa4-2d95-4158-8ac6-92b145e05e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.utils import shuffle\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import Utils.Time_Series_Classification_Helpers as ts_helpers\n",
    "import Utils.Classification_Helpers as helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebc64c0d-6d6c-4763-9df3-2e548c0e5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  # Use 'None' to display all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49ecfc-b04c-45b5-a328-6ff826897526",
   "metadata": {},
   "source": [
    "# Set up MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "552878fa-4803-4f83-8277-7c0ff79a90ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start MLFlow\n",
    "#!mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfebd2bd-c2ae-4213-92be-9d14b32128a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ef8fb-0483-47c0-a098-4765de47b4f5",
   "metadata": {},
   "source": [
    "# Import and Concatenate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44894145-e4db-4240-8466-9ff4fa84392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_list = [\"293\", \"294\", \"298\"]\n",
    "label_list  = [1, 2, 3, 4, 5, 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6f9205-932a-417c-b6ff-d92f746b710d",
   "metadata": {},
   "source": [
    "## Dataframes that do not depend on folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941a6283-a863-4098-bd77-4dffcaffb3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_and_concatenate_datasets(subject_list, list_of_filenames, parent_directory):\n",
    "    \"\"\"\n",
    "    Import and concatenate feature datasets for each subject.\n",
    "\n",
    "    Args:\n",
    "    - subject_list (list): List of subject names.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Concatenated feature DataFrame.\n",
    "    - list: List of all labels.\n",
    "    \"\"\"\n",
    "    subject_feature_dfs = {}\n",
    "\n",
    "    for subject_idx, subject in enumerate(subject_list):\n",
    "        subject_feature_dfs[subject] = pd.DataFrame()\n",
    "\n",
    "        for data_type in [\"EEG\", \"EMG\"]:\n",
    "            data_frames = []\n",
    "\n",
    "            for file in list_of_filenames:\n",
    "                path = os.path.join(str(parent_directory), \"Features\", str(subject), str(data_type), file)\n",
    "                if os.path.exists(path):\n",
    "                    data_frames.append(pd.read_csv(path))\n",
    "\n",
    "            df_both_data_types = pd.concat(data_frames, axis=1)\n",
    "\n",
    "            if not subject_feature_dfs[subject].empty:\n",
    "                subject_feature_dfs[subject] = pd.concat([subject_feature_dfs[subject], df_both_data_types], axis=1).drop(columns=['Unnamed: 0'], inplace=False)\n",
    "                subject_feature_dfs[subject] = helpers.keep_first_duplicate_columns(subject_feature_dfs[subject])\n",
    "            else:\n",
    "                df_both_data_types = helpers.keep_first_duplicate_columns(df_both_data_types)\n",
    "                subject_feature_dfs[subject] = df_both_data_types.drop(columns=['Unnamed: 0'], inplace=False)\n",
    "\n",
    "        subject_feature_dfs[subject][\"Subject\"] = subject_idx\n",
    "\n",
    "    feature_df = pd.concat(subject_feature_dfs.values(), ignore_index=True)\n",
    "\n",
    "    # For duplicate columns, only keep one\n",
    "    feature_df = helpers.keep_first_duplicate_columns(feature_df)\n",
    "    \n",
    "    return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2209dc-3870-4f09-b6aa-e46f50298cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_filenames = [\"Topological_Summary_Statistics.csv\", \"Advanced_Features.csv\", \"Signature_Statistics.csv\"]\n",
    "\n",
    "feature_df = import_and_concatenate_datasets(subject_list, list_of_filenames, parent_directory = \"\")\n",
    "\n",
    "all_labels = feature_df[\"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d18abb-6f9a-4b13-b965-0d745e311dab",
   "metadata": {},
   "source": [
    "## Dataframes that DO depend on folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2e758e-9f50-4db2-80e2-ca091cb0e6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_filenames = [\"Vectorization_Features.csv\"]\n",
    "\n",
    "fold_dependant_feature_df = ts_helpers.import_and_concatenate_datasets(subject_list, list_of_filenames, parent_directory = \"\")\n",
    "\n",
    "\n",
    "list_of_filenames = [\"Vectorization_Features_for_Final_Test.csv\"]\n",
    "\n",
    "fold_dependant_final_test_feature_df = ts_helpers.import_and_concatenate_datasets(subject_list, list_of_filenames, parent_directory = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c4ac3-9d48-4ab1-a76c-93168323ade0",
   "metadata": {},
   "source": [
    "# Experiments with Single Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eafcd9c0-7e26-4dc1-8bca-4ce7b1df4bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1955 features in the main dataframe.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are \"+str(len(feature_df.columns))+\" features in the main dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069fcb66-1b39-4cc4-8106-8bebe6898c68",
   "metadata": {},
   "source": [
    "## Save features for Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ad0052-bb36-460a-8e57-e6525645b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv(\"Features/All_Features.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c438e6-7241-4d14-b4d5-0b2243f636b3",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b91f048c-41a3-4423-a588-776234da857e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There now are 1955 features in the main dataframe.\n"
     ]
    }
   ],
   "source": [
    "#list_of_strings_in_column_name = [\"Persistence_image_Statistic_\", \"Persistence_Landscape_Statistic_\", \"Vectorization\"]\n",
    "list_of_strings_in_column_name = [\"Nothing\"]\n",
    "\n",
    "feature_df = helpers.remove_columns_with_str(feature_df, list_of_strings_in_column_name)\n",
    "\n",
    "print(\"There now are \"+str(len(feature_df.columns))+\" features in the main dataframe.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6cf73d-4845-4a3f-ab50-74282e118ba8",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a17cb31f-aff2-4dc6-9924-16ffac33e76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d3ba5fe-33ba-43fa-b903-60030ad7442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO This can be in the helper file as well\n",
    "train_indices, validation_indices, test_indices = helpers.load_folds(subject_list, parent_directory = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e19d74b-76b3-4908-bdf9-c4733a1c9ae6",
   "metadata": {},
   "source": [
    "## Features that do not depend on folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce54a8e-a97a-4744-a908-c79c71cd2dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_dfs_all_folds, train_labels_all_folds = helpers.filter_dataframe_with_indices(feature_df, train_indices, label_list)\n",
    "validation_features_dfs_all_folds, validation_labels_all_folds = helpers.filter_dataframe_with_indices(feature_df, validation_indices, label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a046c-c14d-4ce3-803b-6ea6b674cb41",
   "metadata": {},
   "source": [
    "## Fold-dependant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c95e0fe4-d9d8-45df-bdfd-8628e1b37740",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold_dependant_features_dfs_all_folds, _ = helpers.filter_fold_dependant_dataframe_with_indices(fold_dependant_feature_df, train_indices, label_list)\n",
    "validation_fold_dependant_features_dfs_all_folds, _ = helpers.filter_fold_dependant_dataframe_with_indices(fold_dependant_feature_df, validation_indices, label_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fa5ea7-ed61-4a0c-86c7-58abaa35c331",
   "metadata": {},
   "source": [
    "## Reformat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3fe5e8e-f7eb-4532-976f-9b8c4cdc3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out if you want to leave out fold-dependant features\n",
    "train_features_dfs_all_folds = helpers.combine_all_features(train_features_dfs_all_folds, train_fold_dependant_features_dfs_all_folds)\n",
    "validation_features_dfs_all_folds = helpers.combine_all_features(validation_features_dfs_all_folds, validation_fold_dependant_features_dfs_all_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf2cc5b-770b-4ddc-835e-744e33435e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and validation sets\n",
    "# TO DO What is this step for?\n",
    "X_train, y_train, X_test, y_test = helpers.initialize_fold_dicts(train_features_dfs_all_folds, train_labels_all_folds, validation_features_dfs_all_folds, validation_labels_all_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e5d46-116c-4bd9-9a01-b71d2798616d",
   "metadata": {},
   "source": [
    "# MLFLow & Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14ba4f04-3db8-4cc8-a980-6a20407b2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, validation_indices, test_indices = helpers.load_folds(subject_list, parent_directory=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c13e49d-8705-4570-b876-821971d251ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train data of all folds such that the function GridSearchCV works on them\n",
    "train_dfs = [X_train[0], X_train[1],  X_train[2],  X_train[3],  X_train[4]]\n",
    "concatenated_X_train = pd.concat(train_dfs, ignore_index=True)\n",
    "test_dfs = [X_test[0], X_test[1],  X_test[2],  X_test[3],  X_test[4]]\n",
    "concatenated_X_test = pd.concat(test_dfs, ignore_index=True)\n",
    "concatenated_X = pd.concat([concatenated_X_train, concatenated_X_test], ignore_index=True)\n",
    "\n",
    "concatenated_y_train = y_train[0] + y_train[1] +  y_train[2] + y_train[3] + y_train[4]\n",
    "concatenated_y_test = y_test[0] + y_test[1] +  y_test[2] + y_test[3] + y_test[4]\n",
    "concatenated_y = concatenated_y_train + concatenated_y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcf03919-4262-45c5-9fb0-63776e6a84e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end indices for each fold's training and test sets\n",
    "train_test_splits = []\n",
    "current_train_start = 0\n",
    "current_test_start = sum(len(X_train[i]) for i in range(5))\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    train_len = len(X_train[i])\n",
    "    test_len = len(X_test[i])\n",
    "\n",
    "    indices_for_training = np.arange(current_train_start, current_train_start + train_len)\n",
    "    indices_for_testing = np.arange(current_test_start, current_test_start + test_len)\n",
    "    \n",
    "    train_test_splits.append((indices_for_training, indices_for_testing))\n",
    "    \n",
    "    current_train_start += train_len\n",
    "    current_test_start += test_len\n",
    "\n",
    "# Define a custom generator for cross-validation\n",
    "class CustomCV:\n",
    "    def __init__(self, train_test_splits):\n",
    "        self.train_test_splits = train_test_splits\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for train_idx, test_idx in self.train_test_splits:\n",
    "            yield train_idx, test_idx\n",
    "    \n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return len(self.train_test_splits)\n",
    "\n",
    "# Create the custom cross-validation object\n",
    "custom_cv = CustomCV(train_test_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b97438-9ecb-463f-af84-dee90a7fe26e",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eac232c6-22a7-4412-b1f2-48edb47335fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.951 total time=   5.0s\n",
      "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.939 total time=   5.2s\n",
      "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.956 total time=   5.2s\n",
      "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.954 total time=   4.9s\n",
      "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.957 total time=   5.1s\n",
      "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.951 total time=   6.7s\n",
      "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.937 total time=   6.8s\n",
      "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.957 total time=   6.8s\n",
      "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.954 total time=   6.5s\n",
      "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.957 total time=   6.8s\n",
      "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.951 total time=   8.4s\n",
      "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.939 total time=   8.5s\n",
      "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.955 total time=   8.5s\n",
      "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.954 total time=   8.3s\n",
      "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.957 total time=   8.6s\n",
      "[CV 1/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.951 total time=  10.1s\n",
      "[CV 2/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.937 total time=  10.1s\n",
      "[CV 3/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.956 total time=  10.2s\n",
      "[CV 4/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.951 total time=   9.9s\n",
      "[CV 5/5] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.955 total time=  10.2s\n",
      "[CV 1/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.951 total time=   5.0s\n",
      "[CV 2/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.939 total time=   5.3s\n",
      "[CV 3/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.956 total time=   5.1s\n",
      "[CV 4/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.954 total time=   4.9s\n",
      "[CV 5/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.957 total time=   5.0s\n",
      "[CV 1/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.951 total time=   6.6s\n",
      "[CV 2/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.937 total time=   7.0s\n",
      "[CV 3/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.957 total time=   6.8s\n",
      "[CV 4/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.954 total time=   6.7s\n",
      "[CV 5/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.957 total time=   6.9s\n",
      "[CV 1/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.951 total time=   8.4s\n",
      "[CV 2/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.939 total time=   8.6s\n",
      "[CV 3/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.955 total time=   8.6s\n",
      "[CV 4/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.954 total time=   8.3s\n",
      "[CV 5/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.957 total time=   8.5s\n",
      "[CV 1/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.951 total time=  10.1s\n",
      "[CV 2/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.937 total time=  10.3s\n",
      "[CV 3/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.956 total time=  10.1s\n",
      "[CV 4/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.951 total time=  10.0s\n",
      "[CV 5/5] END max_depth=50, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.955 total time=  10.5s\n",
      "[CV 1/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.951 total time=   5.1s\n",
      "[CV 2/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.939 total time=   5.1s\n",
      "[CV 3/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.956 total time=   5.1s\n",
      "[CV 4/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.954 total time=   5.0s\n",
      "[CV 5/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=300;, score=0.957 total time=   5.1s\n",
      "[CV 1/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.951 total time=   6.7s\n",
      "[CV 2/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.937 total time=   6.9s\n",
      "[CV 3/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.957 total time=   6.8s\n",
      "[CV 4/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.954 total time=   6.6s\n",
      "[CV 5/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=400;, score=0.957 total time=   6.8s\n",
      "[CV 1/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.951 total time=   8.3s\n",
      "[CV 2/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.939 total time=   8.5s\n",
      "[CV 3/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.955 total time=   8.5s\n",
      "[CV 4/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.954 total time=   8.4s\n",
      "[CV 5/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=500;, score=0.957 total time=   8.5s\n",
      "[CV 1/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.951 total time=  10.0s\n",
      "[CV 2/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.937 total time=  10.4s\n",
      "[CV 3/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.956 total time=  10.2s\n",
      "[CV 4/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.951 total time=   9.9s\n",
      "[CV 5/5] END max_depth=70, min_samples_leaf=1, min_samples_split=2, n_estimators=600;, score=0.955 total time=  10.3s\n",
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best Score: 0.9511059252835844\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the parameter grid after first initial manual experiments\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 400],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Result: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
    "\n",
    "\n",
    "# Another parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [None, 20, 30],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Result: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
    "\n",
    "# Another parameter grid for finer tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [300, 400, 500, 600],\n",
    "    'max_depth': [None, 50, 70],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Result: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=custom_cv, scoring='accuracy', verbose = 3)\n",
    "\n",
    "grid_search.fit(concatenated_X, concatenated_y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bf79035-a8a9-4c0a-a26c-30bc5b691a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 1 : 0.9508021390374332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 2 : 0.9387755102040817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 3 : 0.9547900968783638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 4 : 0.9535637149028078\n",
      "Accuracy for fold 5 : 0.9565217391304348\n",
      "Average Accuracy: 0.9508906400306243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piabaronetzky/anaconda3/envs/time-delay-embeddings/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "params = {\"random_state\": 42, \"n_estimators\": 500, \"min_samples_split\": 2}\n",
    "rf = RandomForestClassifier(**params)\n",
    "all_accuracies = []\n",
    "\n",
    "for fold in range(5):\n",
    "    rf.fit(X_train[fold], y_train[fold])\n",
    "    y_pred = rf.predict(X_test[fold])\n",
    "    accuracy = accuracy_score(y_pred, y_test[fold])\n",
    "    all_accuracies.append(accuracy)\n",
    "    print(\"Accuracy for fold\", fold + 1, \":\", accuracy)\n",
    "\n",
    "average_accuracy = np.mean(all_accuracies)\n",
    "print(\"Average Accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa405cc4-7154-426f-a02b-39dc07bf0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_params = params\n",
    "features = X_train[0].columns\n",
    "mlflow_params[\"features\"] = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fef990b7-24b4-4a21-8d16-80fa7a523e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(mlflow_params)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", average_accuracy)\n",
    "    mlflow.log_metric(\"minimal accuracy\",  np.min(all_accuracies))\n",
    "    mlflow.log_metric(\"maximal accuracy\",  np.max(all_accuracies))\n",
    "\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"Random Forest Sleep Data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train[fold].values, rf.predict(X_train[fold].values))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=rf,\n",
    "        artifact_path=\"random_forest-sleep-data\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[fold],\n",
    "        registered_model_name=\"random_forest-sleep-data\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cc605d-75f0-4331-a1ab-f42e778386b7",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d52970d-aba8-46b3-8848-406df4bc6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the labels to adequate labels for XGBoost\n",
    "mapping = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 7: 5}\n",
    "\n",
    "for fold in range(5):\n",
    "    y_train[fold] = [mapping[num] for num in y_train[fold]]\n",
    "    y_test[fold] = [mapping[num] for num in y_test[fold]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d0d8e1d-8f94-40ae-a9e1-9add5c952413",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_X = concatenated_X.drop('Subject', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c606977a-b2fa-42b8-9c0e-b909b02bd6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[CV 1/5] END learning_rate=0.18, max_delta_step=1, max_depth=3, min_child_weight=0, n_estimators=400, subsample=0.5;, score=0.982 total time=  10.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid after first initial manual experiments for approximately finding a sweet spot (with MLFlow, code below)\n",
    "\n",
    "# Main parameter grid for finetuning\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.17, 0.19, 0.21],\n",
    "    \"n_estimators\": [250, 300],\n",
    "    \"max_depth\": [7, 8, 9],\n",
    "    \"min_child_weight\": [0],\n",
    "    \"subsample\": [0.5]\n",
    "}\n",
    "\n",
    "# Result: params = {'learning_rate': 0.19, 'max_depth': 7, 'min_child_weight': 0, 'n_estimators': 300, 'subsample': 0.5}\n",
    "\n",
    "# Another parameter grid for finer tuning\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.19],\n",
    "    \"n_estimators\": [300, 400],\n",
    "    \"max_depth\": [5, 6, 7],\n",
    "    \"min_child_weight\": [0],\n",
    "    \"max_delta_step\": [0, 1],\n",
    "    \"subsample\": [0.5]\n",
    "}\n",
    "\n",
    "# Result: params = {'learning_rate': 0.19, 'max_delta_step': 1, 'max_depth': 5, 'min_child_weight': 0, 'n_estimators': 400, 'subsample': 0.5}\n",
    "\n",
    "\n",
    "# Another parameter grid for finer tuning\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.18, 0.19, 0.2],\n",
    "    \"n_estimators\": [400, 500],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_child_weight\": [0, 1],\n",
    "    \"max_delta_step\": [1, 2],\n",
    "    \"subsample\": [0.5]\n",
    "}\n",
    "\n",
    "# Result:  {'learning_rate': 0.18, 'max_delta_step': 1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 400, 'subsample': 0.5}\n",
    "\n",
    "xb = xgb.XGBClassifier(seed=1)\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(xb, param_grid, cv=custom_cv, scoring='accuracy', verbose = 3)\n",
    "\n",
    "grid_search.fit(concatenated_X, concatenated_y)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11f77b40-9ae1-448d-970c-5353dcc84934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for fold 0.9807486631016042\n",
      "Accuracy for fold 0.9849624060150376\n",
      "Accuracy for fold 0.984930032292788\n",
      "Accuracy for fold 0.9773218142548596\n",
      "Accuracy for fold 0.9826086956521739\n",
      "Mean Accuracy: 0.9821143222632926\n"
     ]
    }
   ],
   "source": [
    "# Manual experiments\n",
    "params = {\"seed\": 1, 'learning_rate': 0.18, 'max_delta_step': 1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 400, 'subsample': 0.5}\n",
    "          \n",
    "#params = best_params\n",
    "xb = xgb.XGBClassifier(**params)\n",
    "all_accuracies = []\n",
    "all_feature_importances = []\n",
    "\n",
    "for fold in range(len(X_train)):\n",
    "    # Remove duplicate columns\n",
    "    X_train[fold] = helpers.keep_first_duplicate_columns(X_train[fold])\n",
    "\n",
    "    xb.fit(X_train[fold], y_train[fold])\n",
    "\n",
    "    X_test[fold] = X_test[fold].loc[:, ~X_test[fold].columns.duplicated()]\n",
    "\n",
    "    y_pred = xb.predict(X_test[fold])\n",
    "    accuracy = accuracy_score(y_pred, y_test[fold])\n",
    "    all_accuracies.append(accuracy)\n",
    "    print(\"Accuracy for fold\", accuracy)\n",
    "\n",
    "    # Get feature importances for the current fold\n",
    "    feature_importances = xb.feature_importances_\n",
    "    all_feature_importances.append(feature_importances)\n",
    "\n",
    "average_accuracy = np.mean(all_accuracies)\n",
    "print(\"Mean Accuracy:\", average_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4196560c-ac33-4df7-9ce1-1a187d4466af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_strings_in_column_name = [\"Persistence_image_Statistic_\", \"Persistence_Landscape_Statistic_\", \"Vectorization\"]\n",
    "\n",
    "# params = {\"seed\": 1, \"learning_rate\": 0.2, \"n_estimators\": 300, \"max_depth\": 8, \"min_child_weight\": 0, \"max_delta_step\": 1, \"subsample\":0.5}\n",
    "# Mean Accuracy: 0.9806019593752222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfc6b3-5291-4298-957e-998f1f2ceccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(mlflow_params)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", average_accuracy)\n",
    "    mlflow.log_metric(\"minimal accuracy\",  np.min(all_accuracies))\n",
    "    mlflow.log_metric(\"maximal accuracy\",  np.max(all_accuracies))\n",
    "\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"XGBoost Sleep Data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train[fold].values, rf.predict(X_train[fold].values))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=rf,\n",
    "        artifact_path=\"xgboost-sleep-data\",\n",
    "        signature=signature,\n",
    "        input_example=X_train[fold],\n",
    "        registered_model_name=\"xgboost-sleep-data\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85d4ab5-f68b-44fc-9b7c-539ae2e1b378",
   "metadata": {},
   "source": [
    "# Final Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1730e067-6c82-43f3-989a-3a79a35f5943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_final_training_and_test_indices(train_indices, validation_indices, subject_list, label_list):\n",
    "    \"\"\"\n",
    "    The new training data consists of the previous training plus the previous validation data\n",
    "    \"\"\"\n",
    "\n",
    "    final_train_indices = {}\n",
    "\n",
    "    for subject in subject_list:\n",
    "\n",
    "        # Initialize\n",
    "        final_train_indices[subject] = {}\n",
    "        \n",
    "        train_indices_for_subject = train_indices[subject]\n",
    "        validation_indices_for_subject = validation_indices[subject]\n",
    "\n",
    "        for label in label_list:\n",
    "            # It does not matter which fold we choose, so simply choose fold 0\n",
    "            if isinstance(train_indices_for_subject[\"Label_\"+str(label)][\"Fold_0\"], (np.ndarray, list)):\n",
    "                train_indices_to_combine = train_indices_for_subject[\"Label_\"+str(label)][\"Fold_0\"]\n",
    "            else:\n",
    "                train_indices_to_combine = [train_indices_for_subject[\"Label_\"+str(label)][\"Fold_0\"]]\n",
    "\n",
    "            if isinstance(validation_indices_for_subject[\"Label_\"+str(label)][\"Fold_0\"], (np.ndarray, list)):\n",
    "                validation_indices_to_combine = validation_indices_for_subject[\"Label_\"+str(label)][\"Fold_0\"]\n",
    "            else:\n",
    "                validation_indices_to_combine = [validation_indices_for_subject[\"Label_\"+str(label)][\"Fold_0\"]]\n",
    "\n",
    "            final_train_indices[subject][\"Label_\"+str(label)] = np.concatenate((train_indices_to_combine, validation_indices_to_combine))\n",
    "\n",
    "\n",
    "    return final_train_indices\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6853b43c-4209-48b4-bf54-070bd7a5c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final training indices\n",
    "final_train_indices = concatenate_final_training_and_test_indices(train_indices, \\\n",
    "                            validation_indices, subject_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c0e6d10-0a1a-40af-9719-3af611f90b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate ATOL features with all other features\n",
    "feature_df = pd.concat([feature_df, fold_dependant_final_test_feature_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b761485c-4cd3-471e-ba79-ad57e414be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = helpers.keep_first_duplicate_columns(feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f9d44e3-9774-48be-9ad2-2cfd515b5e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get final test sets\n",
    "X_train_final, y_train_final, X_test_final, y_test_final = helpers.create_final_input_data_dicts(feature_df,          \n",
    "                                        final_train_indices, test_indices, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143bb698-493b-4c61-a5cb-26c0ed9f0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_accuracies = [] \n",
    "\n",
    "for seed in range(10):\n",
    "\n",
    "    params = {'random_state': seed, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "\n",
    "    rf = RandomForestClassifier(**params)\n",
    "\n",
    "    rf.fit(X_train_final, y_train_final)\n",
    "    y_pred = rf.predict(X_test_final)\n",
    "    accuracy = accuracy_score(y_pred, y_test_final)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    final_accuracies.append(accuracy)\n",
    "\n",
    "print(\"Mean accuracy \" + str(np.mean(final_accuracies)) + \", with standard deviation \"+str(np.std(final_accuracies)) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1797a8c-65c2-47f5-8bc0-9a92d454aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_final = [mapping[num] for num in y_train_final]\n",
    "y_test_final = [mapping[num] for num in y_test_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd8e57a-2118-4da6-8364-858895ea051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9794344473007712\n",
      "Accuracy: 0.9802913453299057\n"
     ]
    }
   ],
   "source": [
    "final_accuracies = [] \n",
    "\n",
    "importances = []\n",
    "\n",
    "for seed in range(10):\n",
    "\n",
    "    params = {\"seed\": seed, 'learning_rate': 0.18, 'max_delta_step': 1, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 400, 'subsample': 0.5}\n",
    "\n",
    "    xb = xgb.XGBClassifier(**params)\n",
    "\n",
    "    xb.fit(X_train_final, y_train_final)\n",
    "    y_pred = xb.predict(X_test_final)\n",
    "    accuracy = accuracy_score(y_pred, y_test_final)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    final_accuracies.append(accuracy)\n",
    "\n",
    "    feature_importances = xb.feature_importances_\n",
    "    importances.append(feature_importances)\n",
    "\n",
    "\n",
    "print(\"Mean accuracy \" + str(np.mean(final_accuracies)) + \", with standard deviation \"+str(np.std(final_accuracies)) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25275729-3eeb-47ae-ad6e-e69b435aff48",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = \"_Persistence_Landscape_Statistic_Mean\"\n",
    "\n",
    "importances_of_single_feature = []\n",
    "\n",
    "for imp in importances: \n",
    "    mask = X_train_final.columns.str.contains(feature)\n",
    "    # Use numpy to get the indices where the mask is True\n",
    "    indices = np.where(mask)[0]\n",
    "    importances_of_single_feature.append(sum(imp[indices]))\n",
    "\n",
    "\n",
    "print(\"Mean feature importance \" + str(np.mean(importances_of_single_feature)) + \", with standard deviation \"+str(np.std(importances_of_single_feature)) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7540ee2-f099-4248-a12a-058c22dbcf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(round(np.mean(importances_of_single_feature),4)) + \" \\pm \"+str(round(np.std(importances_of_single_feature), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b6fea-8f01-4885-addc-7582005d3e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
